September 2007In high school I decided I was going to study philosophy in college.
I had several motives, some more honorable than others.  One of the
less honorable was to shock people.  College was regarded as job
training where I grew up, so studying philosophy seemed an impressively
impractical thing to do.  Sort of like slashing holes in your clothes
or putting a safety pin through your ear, which were other forms
of impressive impracticality then just coming into fashion.But I had some more honest motives as well.  I thought studying
philosophy would be a shortcut straight to wisdom.  All the people
majoring in other things would just end up with a bunch of domain
knowledge.  I would be learning what was really what.I'd tried to read a few philosophy books.  Not recent ones; you
wouldn't find those in our high school library.  But I tried to
read Plato and Aristotle.  I doubt I believed I understood them,
but they sounded like they were talking about something important.
I assumed I'd learn what in college.The summer before senior year I took some college classes.  I learned
a lot in the calculus class, but I didn't learn much in Philosophy
101.  And yet my plan to study philosophy remained intact.  It was
my fault I hadn't learned anything.  I hadn't read the books we
were assigned carefully enough.  I'd give Berkeley's Principles
of Human Knowledge another shot in college.  Anything so admired
and so difficult to read must have something in it, if one could
only figure out what.Twenty-six years later, I still don't understand Berkeley.  I have
a nice edition of his collected works.  Will I ever read it?  Seems
unlikely.The difference between then and now is that now I understand why
Berkeley is probably not worth trying to understand.  I think I see
now what went wrong with philosophy, and how we might fix it.WordsI did end up being a philosophy major for most of college.  It
didn't work out as I'd hoped.  I didn't learn any magical truths
compared to which everything else was mere domain knowledge.  But
I do at least know now why I didn't.  Philosophy doesn't really
have a subject matter in the way math or history or most other
university subjects do.  There is no core of knowledge one must
master.  The closest you come to that is a knowledge of what various
individual philosophers have said about different topics over the
years.  Few were sufficiently correct that people have forgotten
who discovered what they discovered.Formal logic has some subject matter. I took several classes in
logic.  I don't know if I learned anything from them.
[1]
It does seem to me very important to be able to flip ideas around in
one's head: to see when two ideas don't fully cover the space of
possibilities, or when one idea is the same as another but with a
couple things changed.  But did studying logic teach me the importance
of thinking this way, or make me any better at it?  I don't know.There are things I know I learned from studying philosophy.  The
most dramatic I learned immediately, in the first semester of
freshman year, in a class taught by Sydney Shoemaker.  I learned
that I don't exist.  I am (and you are) a collection of cells that
lurches around driven by various forces, and calls itself I.  But
there's no central, indivisible thing that your identity goes with.
You could conceivably lose half your brain and live.  Which means
your brain could conceivably be split into two halves and each
transplanted into different bodies.  Imagine waking up after such
an operation.  You have to imagine being two people.The real lesson here is that the concepts we use in everyday life
are fuzzy, and break down if pushed too hard.  Even a concept as
dear to us as I.  It took me a while to grasp this, but when I
did it was fairly sudden, like someone in the nineteenth century
grasping evolution and realizing the story of creation they'd been
told as a child was all wrong. 
[2]
Outside of math there's a limit
to how far you can push words; in fact, it would not be a bad
definition of math to call it the study of terms that have precise
meanings.  Everyday words are inherently imprecise.  They work well
enough in everyday life that you don't notice.  Words seem to work,
just as Newtonian physics seems to.  But you can always make them
break if you push them far enough.I would say that this has been, unfortunately for philosophy, the
central fact of philosophy.  Most philosophical debates are not
merely afflicted by but driven by confusions over words.  Do we
have free will?  Depends what you mean by "free." Do abstract ideas
exist?  Depends what you mean by "exist."Wittgenstein is popularly credited with the idea that most philosophical
controversies are due to confusions over language.  I'm not sure
how much credit to give him.  I suspect a lot of people realized
this, but reacted simply by not studying philosophy, rather than
becoming philosophy professors.How did things get this way?  Can something people have spent
thousands of years studying really be a waste of time?  Those are
interesting questions.  In fact, some of the most interesting
questions you can ask about philosophy.  The most valuable way to
approach the current philosophical tradition may be neither to get
lost in pointless speculations like Berkeley, nor to shut them down
like Wittgenstein, but to study it as an example of reason gone
wrong.HistoryWestern philosophy really begins with Socrates, Plato, and Aristotle.
What we know of their predecessors comes from fragments and references
in later works; their doctrines could be described as speculative
cosmology that occasionally strays into analysis.  Presumably they
were driven by whatever makes people in every other society invent
cosmologies.
[3]With Socrates, Plato, and particularly Aristotle, this tradition
turned a corner.  There started to be a lot more analysis.  I suspect
Plato and Aristotle were encouraged in this by progress in math.
Mathematicians had by then shown that you could figure things out
in a much more conclusive way than by making up fine sounding stories
about them.  
[4]People talk so much about abstractions now that we don't realize
what a leap it must have been when they first started to.  It was
presumably many thousands of years between when people first started
describing things as hot or cold and when someone asked "what is
heat?"  No doubt it was a very gradual process.  We don't know if
Plato or Aristotle were the first to ask any of the questions they
did.  But their works are the oldest we have that do this on a large
scale, and there is a freshness (not to say naivete) about them
that suggests some of the questions they asked were new to them,
at least.Aristotle in particular reminds me of the phenomenon that happens
when people discover something new, and are so excited by it that
they race through a huge percentage of the newly discovered territory
in one lifetime.  If so, that's evidence of how new this kind of
thinking was. 
[5]This is all to explain how Plato and Aristotle can be very impressive
and yet naive and mistaken.  It was impressive even to ask the
questions they did.  That doesn't mean they always came up with
good answers.  It's not considered insulting to say that ancient
Greek mathematicians were naive in some respects, or at least lacked
some concepts that would have made their lives easier.  So I hope
people will not be too offended if I propose that ancient philosophers
were similarly naive.  In particular, they don't seem to have fully
grasped what I earlier called the central fact of philosophy: that
words break if you push them too far."Much to the surprise of the builders of the first digital computers,"
Rod Brooks wrote, "programs written for them usually did not work."
[6]
Something similar happened when people first started trying
to talk about abstractions.  Much to their surprise, they didn't
arrive at answers they agreed upon.  In fact, they rarely seemed
to arrive at answers at all.They were in effect arguing about artifacts induced by sampling at
too low a resolution.The proof of how useless some of their answers turned out to be is
how little effect they have.  No one after reading Aristotle's
Metaphysics does anything differently as a result.
[7]Surely I'm not claiming that ideas have to have practical applications
to be interesting?  No, they may not have to.  Hardy's boast that
number theory had no use whatsoever wouldn't disqualify it.  But
he turned out to be mistaken.  In fact, it's suspiciously hard to
find a field of math that truly has no practical use.  And Aristotle's
explanation of the ultimate goal of philosophy in Book A of the
Metaphysics implies that philosophy should be useful too.Theoretical KnowledgeAristotle's goal was to find the most general of general principles.
The examples he gives are convincing: an ordinary worker builds
things a certain way out of habit; a master craftsman can do more
because he grasps the underlying principles.  The trend is clear:
the more general the knowledge, the more admirable it is.  But then
he makes a mistake—possibly the most important mistake in the
history of philosophy.  He has noticed that theoretical knowledge
is often acquired for its own sake, out of curiosity, rather than
for any practical need.  So he proposes there are two kinds of
theoretical knowledge: some that's useful in practical matters and
some that isn't.  Since people interested in the latter are interested
in it for its own sake, it must be more noble.  So he sets as his
goal in the Metaphysics the exploration of knowledge that has no
practical use.  Which means no alarms go off when he takes on grand
but vaguely understood questions and ends up getting lost in a sea
of words.His mistake was to confuse motive and result.  Certainly, people
who want a deep understanding of something are often driven by
curiosity rather than any practical need.  But that doesn't mean
what they end up learning is useless.  It's very valuable in practice
to have a deep understanding of what you're doing; even if you're
never called on to solve advanced problems, you can see shortcuts
in the solution of simple ones, and your knowledge won't break down
in edge cases, as it would if you were relying on formulas you
didn't understand.  Knowledge is power.  That's what makes theoretical
knowledge prestigious.  It's also what causes smart people to be
curious about certain things and not others; our DNA is not so
disinterested as we might think.So while ideas don't have to have immediate practical applications
to be interesting, the kinds of things we find interesting will
surprisingly often turn out to have practical applications.The reason Aristotle didn't get anywhere in the Metaphysics was
partly that he set off with contradictory aims: to explore the most
abstract ideas, guided by the assumption that they were useless.
He was like an explorer looking for a territory to the north of
him, starting with the assumption that it was located to the south.And since his work became the map used by generations of future
explorers, he sent them off in the wrong direction as well. 
[8]
Perhaps worst of all, he protected them from both the criticism of
outsiders and the promptings of their own inner compass by establishing
the principle that the most noble sort of theoretical knowledge had
to be useless.The Metaphysics is mostly a failed experiment.  A few ideas from
it turned out to be worth keeping; the bulk of it has had no effect
at all.  The Metaphysics is among the least read of all famous
books.  It's not hard to understand the way Newton's Principia
is, but the way a garbled message is.Arguably it's an interesting failed experiment.  But unfortunately
that was not the conclusion Aristotle's successors derived from
works like the Metaphysics. 
[9]
Soon after, the western world
fell on intellectual hard times.  Instead of version 1s to be
superseded, the works of Plato and Aristotle became revered texts
to be mastered and discussed.  And so things remained for a shockingly
long time.  It was not till around 1600 (in Europe, where the center
of gravity had shifted by then) that one found people confident
enough to treat Aristotle's work as a catalog of mistakes.  And
even then they rarely said so outright.If it seems surprising that the gap was so long, consider how little
progress there was in math between Hellenistic times and the
Renaissance.In the intervening years an unfortunate idea took hold:  that it
was not only acceptable to produce works like the Metaphysics,
but that it was a particularly prestigious line of work, done by a
class of people called philosophers.  No one thought to go back and
debug Aristotle's motivating argument.  And so instead of correcting
the problem Aristotle discovered by falling into it—that you can
easily get lost if you talk too loosely about very abstract ideas—they 
continued to fall into it.The SingularityCuriously, however, the works they produced continued to attract
new readers.  Traditional philosophy occupies a kind of singularity
in this respect.  If you write in an unclear way about big ideas,
you produce something that seems tantalizingly attractive to
inexperienced but intellectually ambitious students.  Till one knows
better, it's hard to distinguish something that's hard to understand
because the writer was unclear in his own mind from something like
a mathematical proof that's hard to understand because the ideas
it represents are hard to understand.  To someone who hasn't learned
the difference, traditional philosophy seems extremely attractive:
as hard (and therefore impressive) as math, yet broader in scope.
That was what lured me in as a high school student.This singularity is even more singular in having its own defense
built in.  When things are hard to understand, people who suspect
they're nonsense generally keep quiet.  There's no way to prove a
text is meaningless.  The closest you can get is to show that the
official judges of some class of texts can't distinguish them from
placebos. 
[10]And so instead of denouncing philosophy, most people who suspected
it was a waste of time just studied other things.  That alone is
fairly damning evidence, considering philosophy's claims.  It's
supposed to be about the ultimate truths. Surely all smart people
would be interested in it, if it delivered on that promise.Because philosophy's flaws turned away the sort of people who might
have corrected them, they tended to be self-perpetuating.  Bertrand
Russell wrote in a letter in 1912:

  Hitherto the people attracted to philosophy have been mostly those
  who loved the big generalizations, which were all wrong, so that
  few people with exact minds have taken up the subject.
[11]

His response was to launch Wittgenstein at it, with dramatic results.I think Wittgenstein deserves to be famous not for the discovery
that most previous philosophy was a waste of time, which judging
from the circumstantial evidence must have been made by every smart
person who studied a little philosophy and declined to pursue it
further, but for how he acted in response.
[12]
Instead of quietly
switching to another field, he made a fuss, from inside.  He was
Gorbachev.The field of philosophy is still shaken from the fright Wittgenstein
gave it. 
[13]
Later in life he spent a lot of time talking about
how words worked.  Since that seems to be allowed, that's what a
lot of philosophers do now.  Meanwhile, sensing a vacuum in the
metaphysical speculation department, the people who used to do
literary criticism have been edging Kantward, under new names like
"literary theory," "critical theory," and when they're feeling
ambitious, plain "theory."  The writing is the familiar word salad:

  Gender is not like some of the other grammatical modes which
  express precisely a mode of conception without any reality that
  corresponds to the conceptual mode, and consequently do not express
  precisely something in reality by which the intellect could be
  moved to conceive a thing the way it does, even where that motive
  is not something in the thing as such.
  [14]

The singularity I've described is not going away.  There's a market
for writing that sounds impressive and can't be disproven. There
will always be both supply and demand.  So if one group abandons
this territory, there will always be others ready to occupy it.A ProposalWe may be able to do better.  Here's an intriguing possibility.
Perhaps we should do what Aristotle meant to do, instead of what
he did.  The goal he announces in the Metaphysics seems one worth
pursuing: to discover the most general truths.  That sounds good.
But instead of trying to discover them because they're useless,
let's try to discover them because they're useful.I propose we try again, but that we use that heretofore despised
criterion, applicability, as a guide to keep us from wondering
off into a swamp of abstractions.  Instead of trying to answer the
question:

  What are the most general truths?

let's try to answer the question

  Of all the useful things we can say, which are the most general?

The test of utility I propose is whether we cause people who read
what we've written to do anything differently afterward.  Knowing
we have to give definite (if implicit) advice will keep us from
straying beyond the resolution of the words we're using.The goal is the same as Aristotle's; we just approach it from a
different direction.As an example of a useful, general idea, consider that of the
controlled experiment.  There's an idea that has turned out to be
widely applicable.  Some might say it's part of science, but it's
not part of any specific science; it's literally meta-physics (in
our sense of "meta").   The idea of evolution is another. It turns
out to have quite broad applications—for example, in genetic
algorithms and even product design.  Frankfurt's distinction between
lying and bullshitting seems a promising recent example.
[15]These seem to me what philosophy should look like: quite general
observations that would cause someone who understood them to do
something differently.Such observations will necessarily be about things that are imprecisely
defined.  Once you start using words with precise meanings, you're
doing math.  So starting from utility won't entirely solve the
problem I described above—it won't flush out the metaphysical
singularity.  But it should help.  It gives people with good
intentions a new roadmap into abstraction.  And they may thereby
produce things that make the writing of the people with bad intentions
look bad by comparison.One drawback of this approach is that it won't produce the sort of
writing that gets you tenure.  And not just because it's not currently
the fashion.  In order to get tenure in any field you must not
arrive at conclusions that members of tenure committees can disagree
with.  In practice there are two kinds of solutions to this problem.
In math and the sciences, you can prove what you're saying, or at
any rate adjust your conclusions so you're not claiming anything
false ("6 of 8 subjects had lower blood pressure after the treatment").
In the humanities you can either avoid drawing any definite conclusions
(e.g. conclude that an issue is a complex one), or draw conclusions
so narrow that no one cares enough to disagree with you.The kind of philosophy I'm advocating won't be able to take either
of these routes.  At best you'll be able to achieve the essayist's
standard of proof, not the mathematician's or the experimentalist's.
And yet you won't be able to meet the usefulness test without
implying definite and fairly broadly applicable conclusions.  Worse
still, the usefulness test will tend to produce results that annoy
people: there's no use in telling people things they already believe,
and people are often upset to be told things they don't.Here's the exciting thing, though.  Anyone can do this.  Getting
to general plus useful by starting with useful and cranking up the
generality may be unsuitable for junior professors trying to get
tenure, but it's better for everyone else, including professors who
already have it.  This side of the mountain is a nice gradual slope.
You can start by writing things that are useful but very specific,
and then gradually make them more general.  Joe's has good burritos.
What makes a good burrito?  What makes good food?  What makes
anything good?  You can take as long as you want.  You don't have
to get all the way to the top of the mountain.  You don't have to
tell anyone you're doing philosophy.If it seems like a daunting task to do philosophy, here's an
encouraging thought.  The field is a lot younger than it seems.
Though the first philosophers in the western tradition lived about
2500 years ago, it would be misleading to say the field is 2500
years old, because for most of that time the leading practitioners
weren't doing much more than writing commentaries on Plato or
Aristotle while watching over their shoulders for the next invading
army.  In the times when they weren't, philosophy was hopelessly
intermingled with religion.  It didn't shake itself free till a
couple hundred years ago, and even then was afflicted by the
structural problems I've described above.  If I say this, some will
say it's a ridiculously overbroad and uncharitable generalization,
and others will say it's old news, but here goes: judging from their
works, most philosophers up to the present have been wasting their
time.  So in a sense the field is still at the first step. 
[16]That sounds a preposterous claim to make.  It won't seem so
preposterous in 10,000 years.  Civilization always seems old, because
it's always the oldest it's ever been.  The only way to say whether
something is really old or not is by looking at structural evidence,
and structurally philosophy is young; it's still reeling from the
unexpected breakdown of words.Philosophy is as young now as math was in 1500.  There is a lot
more to discover.Notes
[1]
In practice formal logic is not much use, because despite
some progress in the last 150 years we're still only able to formalize
a small percentage of statements.  We may never do that much better,
for the same reason 1980s-style "knowledge representation" could
never have worked; many statements may have no representation more
concise than a huge, analog brain state.[2]
It was harder for Darwin's contemporaries to grasp this than
we can easily imagine.  The story of creation in the Bible is not
just a Judeo-Christian concept; it's roughly what everyone must
have believed since before people were people.  The hard part of
grasping evolution was to realize that species weren't, as they
seem to be, unchanging, but had instead evolved from different,
simpler organisms over unimaginably long periods of time.Now we don't have to make that leap.  No one in an industrialized
country encounters the idea of evolution for the first time as an
adult.  Everyone's taught about it as a child, either as truth or
heresy.[3]
Greek philosophers before Plato wrote in verse.  This must
have affected what they said.  If you try to write about the nature
of the world in verse, it inevitably turns into incantation.  Prose
lets you be more precise, and more tentative.[4]
Philosophy is like math's
ne'er-do-well brother.  It was born when Plato and Aristotle looked
at the works of their predecessors and said in effect "why can't
you be more like your brother?"  Russell was still saying the same
thing 2300 years later.Math is the precise half of the most abstract ideas, and philosophy
the imprecise half.  It's probably inevitable that philosophy will
suffer by comparison, because there's no lower bound to its precision.
Bad math is merely boring, whereas bad philosophy is nonsense.  And
yet there are some good ideas in the imprecise half.[5]
Aristotle's best work was in logic and zoology, both of which
he can  be said to have invented.  But the most dramatic departure
from his predecessors was a new, much more analytical style of
thinking.  He was arguably the first scientist.[6]
Brooks, Rodney, Programming in Common Lisp, Wiley, 1985, p.
94.[7]
Some would say we depend on Aristotle more than we realize,
because his ideas were one of the ingredients in our common culture.
Certainly a lot of the words we use have a connection with Aristotle,
but it seems a bit much to suggest that we wouldn't have the concept
of the essence of something or the distinction between matter and
form if Aristotle hadn't written about them.One way to see how much we really depend on Aristotle would be to
diff European culture with Chinese: what ideas did European culture
have in 1800 that Chinese culture didn't, in virtue of Aristotle's
contribution?[8]
The meaning of the word "philosophy" has changed over time.
In ancient times it covered a broad range of topics, comparable in
scope to our "scholarship" (though without the methodological
implications).  Even as late as Newton's time it included what we
now call "science."  But core of the subject today is still what
seemed to Aristotle the core: the attempt to discover the most
general truths.Aristotle didn't call this "metaphysics."  That name got assigned
to it because the books we now call the Metaphysics came after
(meta = after) the Physics in the standard edition of Aristotle's
works compiled by Andronicus of Rhodes three centuries later.  What
we call "metaphysics" Aristotle called "first philosophy."[9]
Some of Aristotle's immediate successors may have realized
this, but it's hard to say because most of their works are lost.[10]
Sokal, Alan, "Transgressing the Boundaries: Toward a Transformative
Hermeneutics of Quantum Gravity," Social Text 46/47, pp. 217-252.Abstract-sounding nonsense seems to be most attractive when it's
aligned with some axe the audience already has to grind.  If this
is so we should find it's most popular with groups that are (or
feel) weak.  The powerful don't need its reassurance.[11]
Letter to Ottoline Morrell, December 1912.  Quoted in:Monk, Ray, Ludwig Wittgenstein: The Duty of Genius, Penguin, 1991,
p. 75.[12]
A preliminary result, that all metaphysics between Aristotle
and 1783 had been a waste of time, is due to I. Kant.[13]
Wittgenstein asserted a sort of mastery to which the inhabitants
of early 20th century Cambridge seem to have been peculiarly
vulnerable—perhaps partly because so many had been raised religious
and then stopped believing, so had a vacant space in their heads
for someone to tell them what to do (others chose Marx or Cardinal
Newman), and partly because a quiet, earnest place like Cambridge
in that era had no natural immunity to messianic figures, just as
European politics then had no natural immunity to dictators.[14]
This is actually from the Ordinatio of Duns Scotus (ca.
1300), with "number" replaced by "gender."  Plus ca change.Wolter, Allan (trans), Duns Scotus: Philosophical Writings, Nelson,
1963, p. 92.[15]
Frankfurt, Harry, On Bullshit,  Princeton University Press,
2005.[16]
Some introductions to philosophy now take the line that
philosophy is worth studying as a process rather than for any
particular truths you'll learn.  The philosophers whose works they
cover would be rolling in their graves at that.  They hoped they
were doing more than serving as examples of how to argue: they hoped
they were getting results.  Most were wrong, but it doesn't seem
an impossible hope.This argument seems to me like someone in 1500 looking at the lack
of results achieved by alchemy and saying its value was as a process.
No, they were going about it wrong.  It turns out it is possible
to transmute lead into gold (though not economically at current
energy prices), but the route to that knowledge was to
backtrack and try another approach.Thanks to Trevor Blackwell, Paul Buchheit, Jessica Livingston, 
Robert Morris, Mark Nitzberg, and Peter Norvig for reading drafts of this.